{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controllable Text Summarization via Prompt Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file is a template for the different model inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guiding LLMs to generate summaries of specific lengths and format (e.g., five sentences, 280 words, bullet list) poses a considerable challenge. This study seeks to experiment with prompt injection strategies aimed at directing LLMs towards fulfilling user-requested summary accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt injection is a technique that involves adding a prompt to the input of a language model to guide its output. The prompt can be a sentence, a paragraph, or a set of instructions that the model should follow. In this study, we will experiment with different prompt injection strategies to guide the model to generate summaries of specific lengths and formats.\n",
    "\n",
    "We mainly focus on the following prompt injection strategies:\n",
    "\n",
    "Reasoning-based prompts: \n",
    "-   Generate a text {10-20-30} times shorter\n",
    "-   Generate a text of length eâ€¢ln^2 w.r.t. the number of words in the document\n",
    "\n",
    "Instruction-based prompts:\n",
    "-   Generate a summary of {50-75-100-125-150-175-200} words\n",
    "-   Generate a 1-sentence summary\n",
    "-   Generate a bullet point of 3-5-10 items summarizing the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code that should be modified for each model will be marked with a comment like this:\n",
    "- /*********************************/\n",
    "- #***MODIFY THIS FOR EACH MODEL***\n",
    "- /**********************************/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and library installs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "# *************************************\n",
    "# For some models the following libraries gave us errors so we recommend to check the huggingface documentation for the latest installation instructions for each model.\n",
    "!pip3 install --upgrade transformers optimum\n",
    "# If using PyTorch 2.1 + CUDA 12.x:\n",
    "!pip3 install --upgrade auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import math\n",
    "import transformers\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have a dataset file named 'pubmed.pkl' not in the current directory, please change the path accordingly. The dataset file should be a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from file \n",
    "with open('pubmed.pkl', 'rb') as f: \n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the length of each summary and of each text in two separate histograms as subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].hist([len(x) for x in dataset['summary']], bins=20)\n",
    "axs[0].set_title('Summary length')\n",
    "axs[1].hist([len(x) for x in dataset['document']], bins=20)\n",
    "axs[1].set_title('Text length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some example prompts which we use for one-shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_example_text = \"The impact of climate change on global weather patterns has become increasingly evident in recent years, with more frequent and intense storms, heatwaves, and wildfires being reported across the world. Scientists warn that without immediate and concerted efforts to reduce greenhouse gas emissions and mitigate the effects of climate change, these extreme weather events will only become more severe, posing significant risks to human societies, economies, and ecosystems.\"\n",
    "prompt_example_summary_one_sentence = \"Climate change is causing more extreme weather events worldwide, and urgent action is needed to reduce greenhouse gas emissions and mitigate its impacts to prevent further escalation of risks to society, economies, and ecosystems.\"\n",
    "prompt_example_summary_3_bullet = \"Automation and AI advancements are transforming the job market, raising worries about job displacement. Proponents highlight the potential for increased productivity, innovation, and the emergence of new job sectors. Successful adaptation requires investment in retraining programs, lifelong learning, and supportive policies for workers transitioning to new roles.\"\n",
    "prompt_example_text_10_percent = \"In recent years, advances in renewable energy technologies have significantly expanded the options for generating clean and sustainable electricity. Solar photovoltaic (PV) panels, wind turbines, and hydroelectric power plants are among the most widely adopted renewable energy sources, offering environmentally friendly alternatives to fossil fuels. These technologies harness natural resources like sunlight, wind, and water to produce electricity without emitting greenhouse gases or other harmful pollutants. As a result, renewable energy has emerged as a key solution to mitigating climate change and reducing dependence on finite fossil fuel resources. Moreover, the declining costs of renewable energy systems have made them increasingly competitive with traditional energy sources, driving widespread adoption and investment in renewable energy infrastructure worldwide.\"\n",
    "prompt_example_summary_10_percent = \"Renewable energy technologies like solar, wind, and hydro are gaining traction as clean alternatives to fossil fuels, offering environmentally friendly electricity generation without greenhouse gas emissions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the prompts used for zero-shot inference, both the \"reasoning-based\" and \"instruction-based\" prompts:\n",
    "\n",
    "- prompts_before: These are the prompts that are used to guide the model to generate a summary of a specific length or format and are put before the document.\n",
    "- prompts_after: These are the prompts that are put after the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts before are defined as a dictionary with the following elements:\n",
    "- key (str): The prompt name\n",
    "- value (dict): The prompt dictionary with the following elements:\n",
    "    - prompt_type (str): The prompt type. It can be 'percent', 'lenght', 'sentence' or 'bullet'\n",
    "    - value (int): A value that is used by the corresponding prompt type check function. Changes meaning depending on the prompt type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_before = {\n",
    "                'Generate a summary 10 times shorter for the following text: ' : {\"prompt_type\" : \"percent\", \"value\" : 10},\n",
    "                'Generate a summary 20 times shorter for the following text: ' : {\"prompt_type\" : \"percent\", \"value\" : 20},\n",
    "                'Generate a summary 30 times shorter for the following text: ' : {\"prompt_type\" : \"percent\", \"value\" : 30},\n",
    "                'Generate a summary of lenght 50 words: ' : {\"prompt_type\" : \"length\", \"value\" : 50},\n",
    "                'Generate a summary of lenght 75 words: ' : {\"prompt_type\" : \"length\", \"value\" : 75},\n",
    "                'Generate a summary of lenght 100 words: ' : {\"prompt_type\" : \"length\", \"value\" : 100},\n",
    "                'Generate a summary of lenght 125 words: ' : {\"prompt_type\" : \"length\", \"value\" : 125},\n",
    "                'Generate a summary of lenght 150 words: ' : {\"prompt_type\" : \"length\", \"value\" : 150},\n",
    "                'Generate a summary of lenght 175 words: ' : {\"prompt_type\" : \"length\", \"value\" : 175},\n",
    "                'Generate a summary of lenght 200 words: ' : {\"prompt_type\" : \"length\", \"value\" : 200},\n",
    "                'Summarise this text in 1 sentence: ' : {\"prompt_type\" : \"sentence\", \"value\" : 1},\n",
    "                'Summarise this text in 5 sentences: ' : {\"prompt_type\" : \"sentence\", \"value\" : 5},\n",
    "                'Summarise this text in 10 sentences: ' : {\"prompt_type\" : \"sentence\", \"value\" : 10},\n",
    "                'Summarise this text in 3 bullet-points: ' : {\"prompt_type\" : \"bullet\", \"value\" : 3},\n",
    "                'Summarise this text in 5 bullet-points: ' : {\"prompt_type\" : \"bullet\", \"value\" : 5},\n",
    "                'Summarise this text in 10 bullet-points: ' : {\"prompt_type\" : \"bullet\", \"value\" : 10},\n",
    "                'Summarise this text in 1 sentence.\\n Text: ' : {\"prompt_type\" : \"sentence\", \"value\" : 1},\n",
    "                'Summarise this text in 5 sentences.\\n Text: ' : {\"prompt_type\" : \"sentence\", \"value\" : 5},\n",
    "                'Summarise this text in 10 sentences.\\n Text: ' : {\"prompt_type\" : \"sentence\", \"value\" : 10}\n",
    "                # ****************************************\n",
    "                # To add more prompts, use the following format:\n",
    "                # 'Prompt: ' : {\"prompt_type\" : \"type\", \"value\" : value},\n",
    "                # ****************************************\n",
    "                }\n",
    "\n",
    "prompts_after = [' Summary: '\n",
    "                 # ****************************************\n",
    "                 # To add more prompts, use the following format:\n",
    "                    # ' prompt: ' ,\n",
    "                 # ****************************************\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization using nltk library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents and summaries\n",
    "nltk.download('punkt')\n",
    "tokenized_dataframe = pd.DataFrame()\n",
    "tokenized_dataframe['summary'] = dataset['summary'].apply(word_tokenize)\n",
    "tokenized_dataframe['document'] = dataset['document'].apply(word_tokenize)\n",
    "tokenized_dataframe['document_len'] = tokenized_dataframe['document'].apply(len)\n",
    "tokenized_dataframe['original_summary'] = dataset['summary']\n",
    "tokenized_dataframe['original_document'] = dataset['document']\n",
    "\n",
    "tokenized_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents: \", len(tokenized_dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering based on max context length for the model:\n",
    "\n",
    "model_max_context_length is a dictionary that contains the maximum length of the input that the model can handle. It is defined as a dictionary with the following elements:\n",
    "- key (str): The model name\n",
    "- value (int): The maximum length of the input that the model can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**************************************************\n",
    "model_max_context_length = {\"llama2-chat\": 4096,\n",
    "                            \"notus-7b\": 2048}\n",
    "model = \"notus-7b\"\n",
    "#***If the model has a different max context length, add it to the dictionary and also change the model name***\n",
    "#**************************************************\n",
    "max_prompt_length = max([len(x) for x in prompts_before]) + max([len(x) for x in prompts_after])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exclude documents which are too long for the model at hand and could generate errors by truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_context_length = model_max_context_length[model] - max_prompt_length\n",
    "\n",
    "# Filter out documents that are too long\n",
    "tokenized_dataframe = tokenized_dataframe[tokenized_dataframe['document_len'] <= max_context_length]\n",
    "\n",
    "tokenized_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents after filtering: {len(tokenized_dataframe)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from the dataframe rows in which the length of the summary is greater than the length of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from the dataframe rows in which the length of the summary is greater than the length of the document\n",
    "def sanitize(df):\n",
    "    df = df[df['original_summary'].apply(len) <= df['original_document'].apply(len)]\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataframe = sanitize(tokenized_dataframe)\n",
    "\n",
    "print(f\"Number of documents after sanitizing: {len(tokenized_dataframe)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating zero-shot and one-shot prompts based on the data and the prompts before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to generate prompts for the model to generate an output on zero-shot inference and accepts the following parameters:\n",
    "# data: the row of the dataframe containing the document and the summary\n",
    "# special_tokens: the special tokens of the model\n",
    "# prompt_before: the prompt to be used before the document\n",
    "# prompt_after: the prompt to be used after the document\n",
    "# length_based: a boolean value that indicates if the prompt is length-based or not\n",
    "# The function returns a dictionary containing the prompt, the document, the summary, the type of prompt, the value of the prompt and the prompt_before used\n",
    "def prompt_creation_zero_shot(data, special_tokens, prompt_before, prompt_after, length_based=False):\n",
    "    document = data['original_document']\n",
    "    summary = data['original_summary']\n",
    "    doc_len = data['document_len']\n",
    "    \n",
    "    if length_based:\n",
    "        # Length-based prompts are used to generate summaries of different lengths = exln^2\n",
    "        size = math.ceil(math.exp(1) * (math.pow(math.log(doc_len), 2)))\n",
    "        text = f\"Generate a summary of length e*(ln^2): \"\n",
    "        prompt = f\"{special_tokens}{text}{document}{prompt_after}{special_tokens}\"\n",
    "        return {\"prompt\": prompt, \"document\": document, \"summary\": summary, \"prompt_type\": \"lenght\", \"value\": size, \"prompt_before\": text}\n",
    "    \n",
    "    prompt_type = prompts_before[prompt_before][\"prompt_type\"]\n",
    "    value = prompts_before[prompt_before][\"value\"]\n",
    "    prompt = f\"{special_tokens}{prompt_before}{document}{prompts_after}{special_tokens}\"\n",
    "    return {\"prompt\": prompt, \"document\": document, \"summary\": summary, \"prompt_type\": prompt_type, \"value\": value, \"prompt_before\": prompt_before}\n",
    "\n",
    "# This function is used to generate prompts for the model to generate an output on one-shot inference and accepts the following parameters:\n",
    "# data: the row of the dataframe containing the document and the summary\n",
    "# special_tokens: the special tokens of the model\n",
    "# prompt_before: the prompt to be used before the document\n",
    "# prompt_after: the prompt to be used after the document\n",
    "# type_of_prompt: the type of prompt to be used, which can be \"ten percent\", \"one sentence\" or \"three bullet\"\n",
    "# The function returns a tuple containing the prompt and the type of prompt\n",
    "def prompt_creation_one_shot(data, special_tokens, prompt_before, prompt_after, type_of_prompt):\n",
    "    document = data['original_document']\n",
    "    example_after_text = \"Example summary: \"\n",
    "    if type_of_prompt == \"ten percent\":\n",
    "        prompt = f\"{special_tokens}{prompt_before}{prompt_example_text_10_percent}{example_after_text}{prompt_example_summary_10_percent}{prompt_before}{document}{prompt_after}{special_tokens}\"\n",
    "        return prompt, type_of_prompt\n",
    "    if type_of_prompt == \"one sentence\":\n",
    "        prompt = f\"{special_tokens}{prompt_before}{prompt_example_text}{example_after_text}{prompt_example_summary_one_sentence}{prompt_before}{document}{prompt_after}{special_tokens}\"\n",
    "        return prompt, type_of_prompt\n",
    "    if type_of_prompt == \"three bullet\":\n",
    "        prompt = f\"{special_tokens}{prompt_before}{prompt_example_text}{example_after_text}{prompt_example_summary_3_bullet}{prompt_before}{document}{prompt_after}{special_tokens}\"\n",
    "        return prompt, type_of_prompt\n",
    "    return prompt, type_of_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_creation_zero_shot(tokenized_dataframe.iloc[0], \"\", \"Generate a summary 10 times shorter for the following text: \", \" Summary: \", length_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_creation_one_shot(tokenized_dataframe.iloc[0], \"\", \"Summarise this text in 1 sentence.\\n Text: \", \" Summary: \", \"one sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to WandB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### YOUR_API_KEY should be replaced by the personal API key obtainable on Wandb. The code should be then uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************\n",
    "# ***Here you should add the personal api key from wandb, it can be found in the profile settings page***\n",
    "# *******************************\n",
    "#wandb.login(key='YOUR_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************\n",
    "# ***Change project variable to the name of the table you want to create on wandb***\n",
    "# *******************************\n",
    "run = wandb.init(project=\"table-test\")\n",
    "\n",
    "# *******************************\n",
    "# ***Change the columns variable to the columns you want to add to the table***\n",
    "# *******************************\n",
    "my_table = wandb.Table(columns=[\"Model\", \"Prompt\", \"Output\", \"Percent check\", \"Percent\", \"Words check\", \"Words\", \"Sentences check\", \"Sentences\", \"BART score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created ad hoc criteria to assess the correctness of the generated summaries. Specifically, we use NLTK word_tokenize to check the output length and compare it to the input length, following the instruction given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a margin not to be overly specific and allow for small disalignments from the precise expected lengt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************\n",
    "# ***Change here the metrics margin for more/less lenient checks***\n",
    "# *******************************\n",
    "percent_margin = 3 \n",
    "length_margin = 10\n",
    "\n",
    "# Percent checK: True if the output has the correct percent of words compared to the input, False otherwise\n",
    "# Percent: the percent of words in the output compared to the input computed as 1/percent \n",
    "# Input: the input text\n",
    "# Output: the output text\n",
    "# Returns a tuple containing the percent check and the percent\n",
    "def percent_check(input, output, percent):\n",
    "    tokenized_input = word_tokenize(input)\n",
    "    tokenized_output = word_tokenize(output)\n",
    "    if len(tokenized_output) <= (1/percent)*len(tokenized_input) + percent_margin and len(tokenized_output) >= (1/percent)*len(tokenized_input) - percent_margin:\n",
    "        return True, len(tokenized_output)*100/len(tokenized_input)\n",
    "    return False, len(tokenized_output)*100/len(tokenized_input)\n",
    "\n",
    "# Words check: True if the output has the correct number of words, False otherwise\n",
    "# Output: the output text\n",
    "# Length: the length of the output\n",
    "# Returns a tuple containing the words check and the length\n",
    "def length_check(output, length):\n",
    "    tokenized_output = word_tokenize(output)\n",
    "    if len(tokenized_output) <= length + length_margin and len(tokenized_output) >= length - length_margin:\n",
    "        return True, len(tokenized_output)\n",
    "    return False, len(tokenized_output)\n",
    "\n",
    "# Sentences check: True if the output has the correct number of sentences, False otherwise\n",
    "# Output: the output text\n",
    "# Sentence_length: the number of sentences in the output\n",
    "# Returns a tuple containing the sentences check and the number of sentences\n",
    "def sentence_check(output, sentence_length):\n",
    "    # remove bullet points and new lines from the output\n",
    "    output = output.replace(\"\\n\", \"\")\n",
    "    regex = re.compile(r'^\\d+\\.\\s*')\n",
    "    tokenized_output = sent_tokenize(output)\n",
    "    filtered = [i for i in tokenized_output if not regex.match(i)]\n",
    "    if len(filtered) == sentence_length:\n",
    "        return True, len(filtered)\n",
    "    return False, len(filtered)\n",
    "\n",
    "# Bullet check: True if the output has the correct number of bullet points, False otherwise\n",
    "# Output: the output text\n",
    "# Bullet_length: the number of bullet points in the output\n",
    "# Returns a tuple containing the bullet check and the number of bullet points\n",
    "def bullet_check(output, bullet_length):\n",
    "    tokenized_output = output.split(\"\\n\")\n",
    "    # remove empty strings from the list\n",
    "    tokenized_output = list(filter(None, tokenized_output))\n",
    "    if len(tokenized_output) == bullet_length:\n",
    "        return True, len(tokenized_output)\n",
    "    return False, len(tokenized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we should also test the quality of the generated summaries. As we cannot rely on a target summary, we selected BARTScore to evaluate the factualness and semantic similarity of the predicted summary against the source document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_score import BARTScorer\n",
    "\n",
    "scorer = BARTScorer() # If needed change max_length to the maximum length of context the model can handle in bart_score.py \n",
    "# scorer.load(path=\"bart_score.pth\")   # Uncomment this line if you want to use the pretrained BART model and if you have the file in the same directory as this notebook \n",
    "\n",
    "# Compute the BART score for the output\n",
    "# Hypotheses: the output text\n",
    "# References: the input text\n",
    "# Returns the BART score\n",
    "def calculate_bart_score(hypotheses, references):\n",
    "    score = scorer.score(references, hypotheses) \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a small example of BARTScore works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bart_score([\"The quick brown fox jumps over the lazy dog\"], [\"The quick brown fox jumps above the lazy dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing setup: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the testing setup for the experiment: what models we use, what prompts we use, and what documents we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************\n",
    "# ***Change the seed variable for different test configurations, keep the same one for result replication consistency***\n",
    "# *******************************\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# choose n random documents from the dataset where n is the number of prompts\n",
    "n = len(prompts_before)\n",
    "random_docs = tokenized_dataframe.sample(n)\n",
    "\n",
    "# create a list of prompts for zero-shot inference\n",
    "prompts_list = list(prompts_before.keys())\n",
    "zero_shot_prompts = []\n",
    "for i in range(n):\n",
    "    for j in range(len(prompts_after)):\n",
    "        zero_shot_prompts.append(prompt_creation_zero_shot(random_docs.iloc[i], \"\", prompts_list[i], prompts_after[j], length_based=False))\n",
    "zero_shot_prompts.append(prompt_creation_zero_shot(random_docs.iloc[0], \"\", prompts_list[0], prompts_after[0], length_based=True))\n",
    "\n",
    "# create a list of prompts for one-shot inference\n",
    "# *******************************\n",
    "# ***Here we just create three custom ones so be mindfull of changes to prompts_before structure if new prompts are added not at the end**\n",
    "# *******************************\n",
    "one_shot_prompts = []\n",
    "one_shot_prompts.append(prompt_creation_one_shot(random_docs.iloc[0], \"\", prompts_list[0], prompts_after[0], \"ten percent\"))\n",
    "one_shot_prompts.append(prompt_creation_one_shot(random_docs.iloc[0], \"\", prompts_list[11], prompts_after[0], \"one sentence\"))\n",
    "one_shot_prompts.append(prompt_creation_one_shot(random_docs.iloc[0], \"\", prompts_list[14], prompts_after[0], \"three bullet\"))\n",
    "\n",
    "print(\"Zero-shot prompts: \", zero_shot_prompts)\n",
    "print(\"One-shot prompts: \", one_shot_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the zero_shot_inference function, which is used to perform zero-shot inference on the models. The function takes as input the model, the tokenizer, model name and the prompts and returns the number of checks passed, the number of checks failed and average BARTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_inference(prompts, model, tokenizer, model_name_or_path):\n",
    "\n",
    "    checks_passed = 0\n",
    "    checks_failed = 0\n",
    "    average_bart_score = 0\n",
    "\n",
    "    for i in range(n * (len(prompts_after)) + 1):\n",
    "        prompt = prompts[i][\"prompt\"]\n",
    "        document = prompts[i][\"document\"]\n",
    "        summary = prompts[i][\"summary\"]\n",
    "        prompt_type = prompts[i][\"prompt_type\"]\n",
    "        value = prompts[i][\"value\"]\n",
    "        prompt_before = prompts[i][\"prompt_before\"]\n",
    "        # *******************************\n",
    "        # ***Change the prompt_template variable to the desired prompt template for the model***\n",
    "        # *******************************\n",
    "        prompt_template=f'''\n",
    "        <|im_start|>user\n",
    "        {prompt}<|im_end|>\n",
    "        '''\n",
    "        print(prompt_template)\n",
    "        # *******************************\n",
    "        # ***Change model output generation to the one specific of the model***\n",
    "        # *******************************\n",
    "        input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "        encoded_output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=model_max_context_length[\"notus-7b\"])\n",
    "        decoded_output = tokenizer.decode(encoded_output[0])\n",
    "        \n",
    "        # *******************************\n",
    "        # ***Change the output splits based on the desired output, the checks work if everything except the actual summary is cut***\n",
    "        # *******************************\n",
    "        output = decoded_output.split(\"|im_end|>\")[1]\n",
    "        output = output.split(\"</s>\")[0]\n",
    "        output = output.split(\"\\n\", 1)[1]\n",
    "        print(output)\n",
    "\n",
    "        # based on prompt type we do different checks\n",
    "        if prompt_type == \"percent\":\n",
    "            check, percent = percent_check(document, output, value)\n",
    "            bart_score = calculate_bart_score([output], [summary])\n",
    "            average_bart_score += bart_score[0]\n",
    "            if check:\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                checks_failed += 1\n",
    "            my_table.add_data(model_name_or_path, prompt, output, check, percent, None, None, None, None, bart_score[0])\n",
    "        elif prompt_type == \"length\":\n",
    "            check, length = length_check(output, value)\n",
    "            bart_score = calculate_bart_score([output], [summary])\n",
    "            average_bart_score += bart_score[0]\n",
    "            if check:\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                checks_failed += 1\n",
    "            my_table.add_data(model_name_or_path, prompt, output, None, None, check, length, None, None, bart_score[0])\n",
    "        elif prompt_type == \"sentence\":\n",
    "            check, sentences = sentence_check(output, value)\n",
    "            bart_score = calculate_bart_score([output], [summary])\n",
    "            average_bart_score += bart_score[0]\n",
    "            if check:\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                checks_failed += 1\n",
    "            my_table.add_data(model_name_or_path, prompt, output, None, None, None, None, check, sentences, bart_score[0])\n",
    "        elif prompt_type == \"bullet\":\n",
    "            check, bullet = bullet_check(output, value)\n",
    "            bart_score = calculate_bart_score([output], [summary])\n",
    "            average_bart_score += bart_score[0]\n",
    "            if check:\n",
    "                checks_passed += 1\n",
    "            else:\n",
    "                checks_failed += 1\n",
    "            my_table.add_data(model_name_or_path, prompt, output, None, None, None, None, check, bullet, bart_score[0])\n",
    "        else:\n",
    "            print(\"Invalid prompt type\")\n",
    "\n",
    "    average_bart_score = average_bart_score / (n * (len(prompts_after)) + 1)\n",
    "    return checks_passed, checks_failed, average_bart_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the one_shot_inference function, which is used to perform one-shot inference on the models. The function takes as input the model, the tokenizer, model name and the prompts and returns the number of checks passed, the number of checks failed and average BARTScore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We infer the summaries one by one, given a model, possibly its tokenizer, the data and the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then upload the results on wandb using multiple metrics matching the specific prompt request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# Load the model and tokenizer\n",
    "# *******************************\n",
    "# ***Change model_name_or_path to the desired model***\n",
    "# *******************************\n",
    "model_name_or_path = \"TheBloke/notus-7B-v1-GPTQ\"\n",
    "\n",
    "# *******************************\n",
    "# ***Change how to model is loaded accoridingly to its documentation***\n",
    "# *******************************\n",
    "# To use a different branch, change revision if the model has one use for example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "# zero-shot inference\n",
    "checks_passed, checks_failed, average_bart_score = zero_shot_inference(zero_shot_prompts, model, tokenizer, model_name_or_path)\n",
    "print(\"Zero-shot inference checks passed: \", checks_passed)\n",
    "print(\"Zero-shot inference checks failed: \", checks_failed)\n",
    "print(\"Zero-shot inference average BART score: \", average_bart_score)\n",
    "\n",
    "# save the table to wandb\n",
    "run.log({\"Table Name\": my_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
